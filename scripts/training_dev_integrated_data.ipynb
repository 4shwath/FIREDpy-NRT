{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.cuda.amp as amp\n",
    "from torchvision import transforms\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from patchify import patchify\n",
    "import torchvision.models.segmentation as models\n",
    "from segmentation_models_pytorch.metrics import iou_score, accuracy\n",
    "import rioxarray\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "\n",
    "# Suppress the specific RuntimeWarning\n",
    "warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in cast\")\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        smooth = 1e-5\n",
    "        \n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).sum()\n",
    "        dice_coeff = (2. * intersection + smooth) / (inputs.sum() + targets.sum() + smooth)\n",
    "        \n",
    "        return 1 - dice_coeff\n",
    "\n",
    "# Combined loss function\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, weight=0.5):\n",
    "        super().__init__()\n",
    "        self.weight = weight\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "        self.dice_loss = DiceLoss()\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        bce = self.bce_loss(inputs, targets)\n",
    "        dice = self.dice_loss(torch.sigmoid(inputs), targets)\n",
    "        return self.weight * bce + (1 - self.weight) * dice\n",
    "\n",
    "class SegmentationGeotiffDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None, train=True, train_split=0.8, patch_size=(256, 256), stride=None):\n",
    "        self.csv_file = csv_file\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "        self.train_split = train_split\n",
    "        self.patch_size = patch_size\n",
    "        self.stride = stride if stride else patch_size\n",
    "        self.read_filenames_from_csv()\n",
    "        self.split_dataset()\n",
    "\n",
    "        self.class_frequencies = {0: 0, 1: 0}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataset[idx]\n",
    "\n",
    "        # Load lum_change, coh_change, dnbr, and binary_product paths from the row\n",
    "        lum_change_path = row['lum_change']\n",
    "        coh_change_path = row['coh_change']\n",
    "        dnbr_path = row['dnbr']\n",
    "        binary_product_path = row['binary_product']\n",
    "\n",
    "        # Load and process lum_change\n",
    "        lum_change = rioxarray.open_rasterio(lum_change_path).squeeze().values\n",
    "        lum_change = np.array(transforms.ToPILImage()(lum_change).resize((1024, 1024), resample=Image.LANCZOS))\n",
    "\n",
    "        # Load and process coh_change\n",
    "        coh_change = rioxarray.open_rasterio(coh_change_path).squeeze().values\n",
    "        coh_change = np.array(transforms.ToPILImage()(coh_change).resize((1024, 1024), resample=Image.LANCZOS))\n",
    "\n",
    "        # Load and process dnbr\n",
    "        dnbr = rioxarray.open_rasterio(dnbr_path).squeeze().values\n",
    "        dnbr = np.array(transforms.ToPILImage()(dnbr).resize((1024, 1024), resample=Image.LANCZOS))\n",
    "\n",
    "        # Load and process binary_product (round to 0 or 1)\n",
    "        binary_product = rioxarray.open_rasterio(binary_product_path).squeeze().values\n",
    "        binary_product = np.rint(binary_product).astype(np.uint8)\n",
    "        binary_product = np.array(transforms.ToPILImage()(binary_product).resize((1024, 1024), resample=Image.LANCZOS))\n",
    "\n",
    "        # Stack the lum_change, coh_change, and dnbr arrays\n",
    "        stacked_array = np.stack([lum_change, coh_change, dnbr], axis=-1)\n",
    "\n",
    "        # Patchify the stacked array and binary_product\n",
    "        image_patches = patchify(stacked_array, (*self.patch_size, 3), step=(*self.stride, 3))\n",
    "        mask_patches = patchify(binary_product, self.patch_size, step=self.stride)\n",
    "\n",
    "        # Reshape patches\n",
    "        image_patches = image_patches.reshape(-1, *self.patch_size, 3)\n",
    "        mask_patches = mask_patches.reshape(-1, *self.patch_size)\n",
    "\n",
    "        # Convert to tensor\n",
    "        images = [transforms.functional.to_tensor(patch.astype(np.float32)) for patch in image_patches]\n",
    "        masks = [torch.tensor(patch, dtype=torch.long) for patch in mask_patches]\n",
    "\n",
    "        return images, masks\n",
    "    \n",
    "    def read_filenames_from_csv(self):\n",
    "        # Read filenames from CSV file\n",
    "        csv_path = os.path.join(self.csv_file)\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "\n",
    "    def extract_fire_id(self, path):\n",
    "        return int(os.path.basename(path).split('_')[0])\n",
    "\n",
    "    def split_dataset(self):\n",
    "        test_fids = [7123, 7792]\n",
    "        train_files = []\n",
    "        test_files = []\n",
    "\n",
    "        for _, row in self.data.iterrows():\n",
    "            fire_id = self.extract_fire_id(row['lum_change'])\n",
    "            if fire_id in test_fids:\n",
    "                test_files.append(row)\n",
    "            else:\n",
    "                train_files.append(row)\n",
    "        \n",
    "        if self.train:\n",
    "            self.dataset = train_files\n",
    "        else:\n",
    "            self.dataset = test_files\n",
    "    \n",
    "    def get_class_frequencies(self):\n",
    "        return self.class_frequencies\n",
    "\n",
    "def compute_iou(pred, target, num_classes):\n",
    "    pred = pred.cpu().numpy()\n",
    "    target = target.cpu().numpy()\n",
    "\n",
    "    iou_per_class = []\n",
    "    for cls in range(num_classes):\n",
    "        intersection = ((pred == cls) & (target == cls)).sum().item()\n",
    "        union = ((pred == cls) | (target == cls)).sum().item()\n",
    "        \n",
    "        if union != 0:\n",
    "            iou = intersection / union\n",
    "        else:\n",
    "            iou = 0.0\n",
    "        \n",
    "        iou_per_class.append(iou)\n",
    "    \n",
    "    mean_iou = sum(iou_per_class) / num_classes\n",
    "    return mean_iou\n",
    "\n",
    "def compute_accuracy(pred, target):\n",
    "    pred = pred.cpu().numpy()\n",
    "    target = target.cpu().numpy()\n",
    "\n",
    "    correct = (pred == target).sum().item()\n",
    "    total = target.size\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# Load pre-trained DeepLabV3 model\n",
    "model = models.deeplabv3_resnet50(weights=\"DEFAULT\", progress=True)\n",
    "\n",
    "# Replace the final layer with a new layer\n",
    "num_classes = 2  # Number of classes for binary classification\n",
    "model.backbone.conv1 = nn.Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Conv2d(256, num_classes, kernel_size=(1, 1), stride=(1, 1))\n",
    ")\n",
    "\n",
    "# Define optimizer and Dice loss criterion\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "criterion = DiceLoss()\n",
    "\n",
    "# Checkpoint loading\n",
    "checkpoint_dir = '/Bhaltos/ASHWATH/integrated_model_checkpoints_100m_v3/'\n",
    "latest_checkpoint = max(\n",
    "    [f for f in os.listdir(checkpoint_dir) if f.startswith('checkpoint_epoch_') and f.endswith('.pt')],\n",
    "    key=lambda f: int(f.split('_')[-1].replace('.pt', '')),\n",
    "    default=None\n",
    ")\n",
    "if latest_checkpoint:\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, latest_checkpoint)\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "    new_state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
    "    \n",
    "    model.load_state_dict(new_state_dict)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    best_loss = checkpoint['loss']\n",
    "    print(f\"Loaded checkpoint from epoch {start_epoch-1} with loss {best_loss}\")\n",
    "else:\n",
    "    start_epoch = 1\n",
    "    best_loss = float('inf')\n",
    "    print(\"No checkpoint found, starting training from scratch.\")\n",
    "\n",
    "# Define hyperparameters\n",
    "batch_size = 3\n",
    "accumulation_steps = 4\n",
    "num_epochs = 20\n",
    "checkpoint_freq = 1\n",
    "best_loss = float('inf')\n",
    "patience = 5\n",
    "early_stopping_counter = 0\n",
    "\n",
    "train_dataset = SegmentationGeotiffDataset(csv_file=\"/Bhaltos/ASHWATH/metadata_v2.csv\", train=True)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=6, pin_memory=True)\n",
    "\n",
    "test_dataset = SegmentationGeotiffDataset(csv_file=\"/Bhaltos/ASHWATH/metadata_v2.csv\", train=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=6, pin_memory=True)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    device_ids = [0, 1, 2]  # IDs of all available GPUs\n",
    "else:\n",
    "    device_ids = None\n",
    "if not isinstance(model, nn.DataParallel):\n",
    "    model = nn.DataParallel(model, device_ids=device_ids)\n",
    "model = model.to(device)\n",
    "\n",
    "# Function to enable gradient checkpointing\n",
    "def enable_gradient_checkpointing(model):\n",
    "    # Check if the model is wrapped in DataParallel\n",
    "    if isinstance(model, nn.DataParallel):\n",
    "        model = model.module\n",
    "\n",
    "    # Enable gradient checkpointing for ResNet layers\n",
    "    def enable_checkpointing(layer):\n",
    "        if hasattr(layer, 'conv1'):\n",
    "            layer.use_checkpoint = True\n",
    "\n",
    "    # Access the backbone layers\n",
    "    for layer in model.backbone.named_children():\n",
    "        if 'layer' in layer[0]:  # layer1, layer2, layer3, layer4\n",
    "            layer[1].apply(enable_checkpointing)\n",
    "\n",
    "# After model initialization and before training loop\n",
    "enable_gradient_checkpointing(model)\n",
    "\n",
    "# Create GradScaler for mixed precision training\n",
    "scaler = amp.GradScaler()\n",
    "\n",
    "# DataFrame to store metrics\n",
    "columns = ['Epoch', 'Train Loss', 'Test Loss', 'Test IoU', 'Test Accuracy']\n",
    "metrics_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(start_epoch, num_epochs+1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for i, (images, masks) in enumerate(tqdm(train_dataloader, desc=f\"Epoch {epoch}/{num_epochs} - Training\")):\n",
    "        \n",
    "        batch_loss = 0\n",
    "        for img_patches, mask_patches in zip(images, masks):\n",
    "            image_loss = 0\n",
    "            for img, mask in zip(img_patches, mask_patches):\n",
    "                img = img.unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "                mask = mask.unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "\n",
    "                with amp.autocast():\n",
    "                    output = model(img)['out']\n",
    "                    output_probs = nn.Softmax(dim=1)(output)\n",
    "                    patch_loss = criterion(output_probs[:, 1], mask.float())\n",
    "\n",
    "                image_loss += patch_loss\n",
    "\n",
    "                # Clear unnecessary memory\n",
    "                del img, mask, output, output_probs\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            # Average loss for all patches in the image\n",
    "            image_loss /= len(img_patches)\n",
    "            batch_loss += image_loss\n",
    "        \n",
    "        # Average loss for all images in the batch\n",
    "        batch_loss /= len(images)\n",
    "        batch_loss = batch_loss / accumulation_steps\n",
    "\n",
    "        # Use scaler for mixed precision training\n",
    "        scaler.scale(batch_loss).backward()\n",
    "\n",
    "        if (i + 1) % accumulation_steps == 0 or (i + 1) == len(train_dataloader):\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        running_loss += batch_loss.item() * accumulation_steps\n",
    "    \n",
    "    avg_train_loss = running_loss / len(train_dataloader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    iou_scores = []\n",
    "    accuracies = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(test_dataloader, desc=f\"Epoch {epoch}/{num_epochs} - Validation\"):\n",
    "            batch_loss = 0\n",
    "            batch_iou = 0\n",
    "            batch_accuracy = 0\n",
    "            \n",
    "            for img_patches, mask_patches in zip(images, masks):\n",
    "                image_predictions = []\n",
    "                image_masks = []\n",
    "                image_loss = 0\n",
    "                \n",
    "                for img, mask in zip(img_patches, mask_patches):\n",
    "                    img = img.unsqueeze(0).to(device)\n",
    "                    mask = mask.unsqueeze(0).to(device)\n",
    "                    \n",
    "                    with amp.autocast():\n",
    "                        output = model(img)['out']\n",
    "                        output_probs = nn.Softmax(dim=1)(output)\n",
    "                        patch_loss = criterion(output_probs[:, 1], mask.float())\n",
    "                    \n",
    "                    image_loss += patch_loss.item()\n",
    "                    output_pred = torch.argmax(output_probs, dim=1)\n",
    "                    \n",
    "                    image_predictions.append(output_pred.cpu())\n",
    "                    image_masks.append(mask.cpu())\n",
    "\n",
    "                    # Clear unnecessary memory\n",
    "                    del img, mask, output, output_probs\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                image_loss /= len(img_patches)\n",
    "                image_pred = torch.cat(image_predictions, dim=0)\n",
    "                image_mask = torch.cat(image_masks, dim=0)\n",
    "                \n",
    "                # Calculate metrics for the entire image\n",
    "                image_iou = compute_iou(image_pred, image_mask, num_classes)\n",
    "                image_accuracy = compute_accuracy(image_pred, image_mask)\n",
    "                \n",
    "                batch_loss += image_loss\n",
    "                batch_iou += image_iou\n",
    "                batch_accuracy += image_accuracy\n",
    "            \n",
    "            # Average metrics for the batch\n",
    "            batch_loss /= len(images)\n",
    "            batch_iou /= len(images)\n",
    "            batch_accuracy /= len(images)\n",
    "            \n",
    "            test_loss += batch_loss\n",
    "            iou_scores.append(batch_iou)\n",
    "            accuracies.append(batch_accuracy)\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_dataloader)\n",
    "    avg_iou = np.mean(iou_scores)\n",
    "    avg_accuracy = np.mean(accuracies)\n",
    "\n",
    "    # Update metrics DataFrame\n",
    "    new_row = pd.DataFrame({\n",
    "    'Epoch': [epoch],\n",
    "    'Train Loss': [avg_train_loss],\n",
    "    'Test Loss': [avg_test_loss],\n",
    "    'Test IoU': [avg_iou],\n",
    "    'Test Accuracy': [avg_accuracy]\n",
    "    })\n",
    "    metrics_df = pd.concat([metrics_df, new_row], ignore_index=True)\n",
    "\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}] - Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}, IoU: {avg_iou:.4f}, Accuracy: {avg_accuracy:.4f}\")\n",
    "\n",
    "    # Check for early stopping\n",
    "    if avg_test_loss < best_loss:\n",
    "        best_loss = avg_test_loss\n",
    "        early_stopping_counter = 0\n",
    "        \n",
    "        # Save the model checkpoint\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch}.pt')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': best_loss\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        if early_stopping_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Save metrics to CSV\n",
    "metrics_df.to_csv('/Bhaltos/ASHWATH/integrated_100m_training_metrics_v3.csv', index=False)\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aramakrishnan/miniforge3/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found, starting training from scratch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Training:  48%|████▊     | 27/56 [02:25<02:30,  5.19s/it]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.cuda.amp as amp\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import functional as TF\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from patchify import patchify\n",
    "import torchvision.models.segmentation as models\n",
    "from segmentation_models_pytorch.metrics import iou_score, accuracy\n",
    "import rioxarray\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import random\n",
    "import cv2\n",
    "from torch_lr_finder import LRFinder\n",
    "\n",
    "\n",
    "# Suppress the specific RuntimeWarning\n",
    "warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in cast\")\n",
    "\n",
    "class RobustBCELoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super(RobustBCELoss, self).__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        input = torch.clamp(input, self.eps, 1 - self.eps)\n",
    "        loss = nn.functional.binary_cross_entropy_with_logits(input, target, reduction='none')\n",
    "        return torch.mean(torch.nan_to_num(loss, nan=0.0, posinf=0.0, neginf=0.0))\n",
    "\n",
    "class ImprovedRobustBCELoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super(ImprovedRobustBCELoss, self).__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        input = torch.clamp(input, self.eps, 1 - self.eps)\n",
    "        loss = nn.functional.binary_cross_entropy_with_logits(input, target, reduction='none')\n",
    "        \n",
    "        # Mask out NaN values in the target\n",
    "        valid_mask = ~torch.isnan(target)\n",
    "        loss = loss[valid_mask]\n",
    "        \n",
    "        if loss.numel() == 0:\n",
    "            return torch.tensor(0.0, device=input.device, requires_grad=True)\n",
    "        \n",
    "        return loss.mean()\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2, alpha=0.25):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        focal_loss = self.alpha * (1-pt)**self.gamma * bce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-5):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).sum()\n",
    "        total = inputs.sum() + targets.sum()\n",
    "        \n",
    "        if total == 0:\n",
    "            return torch.tensor(0.0).to(inputs.device)\n",
    "        \n",
    "        dice_coeff = (2. * intersection + self.smooth) / (total + self.smooth)\n",
    "        return 1 - dice_coeff\n",
    "\n",
    "# Combined Loss\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, weight=0.5, gamma=2, alpha=0.25):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.weight = weight\n",
    "        self.focal = FocalLoss(gamma, alpha)\n",
    "        self.dice = DiceLoss()\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        focal_loss = self.focal(inputs, targets)\n",
    "        dice_loss = self.dice(inputs, targets)\n",
    "        combined_loss = self.weight * focal_loss + (1 - self.weight) * dice_loss\n",
    "        if torch.isnan(combined_loss):\n",
    "            print(f\"NaN in combined loss. Focal: {focal_loss}, Dice: {dice_loss}\")\n",
    "            return focal_loss if not torch.isnan(focal_loss) else dice_loss\n",
    "        return combined_loss\n",
    "\n",
    "class SegmentationGeotiffDataset(Dataset):\n",
    "    def __init__(self, csv_file, train=True, train_split=0.8, patch_size=(256, 256), stride=None):\n",
    "        self.csv_file = csv_file\n",
    "        self.train = train\n",
    "        self.train_split = train_split\n",
    "        self.patch_size = patch_size\n",
    "        self.stride = stride if stride else patch_size\n",
    "        self.read_filenames_from_csv()\n",
    "        self.split_dataset()\n",
    "        self.class_frequencies = {0: 0, 1: 0}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataset[idx]\n",
    "\n",
    "        # Load and process images (lum_change, coh_change, dnbr)\n",
    "        lum_change = self.load_and_resize(row['lum_change'])\n",
    "        coh_change = self.load_and_resize(row['coh_change'])\n",
    "        dnbr = self.load_and_resize(row['dnbr'])\n",
    "\n",
    "        # Load and process binary_product\n",
    "        binary_product = self.load_and_resize(row['binary_product'], is_mask=True)\n",
    "\n",
    "        # Stack the input images\n",
    "        stacked_array = np.stack([lum_change, coh_change, dnbr], axis=-1)\n",
    "\n",
    "        # Patchify the stacked array and binary_product\n",
    "        image_patches = patchify(stacked_array, (*self.patch_size, 3), step=(*self.stride, 3))\n",
    "        mask_patches = patchify(binary_product, self.patch_size, step=self.stride)\n",
    "\n",
    "        # Reshape patches\n",
    "        image_patches = image_patches.reshape(-1, *self.patch_size, 3)\n",
    "        mask_patches = mask_patches.reshape(-1, *self.patch_size)\n",
    "\n",
    "        # Apply transforms to each patch\n",
    "        transformed_images = []\n",
    "        transformed_masks = []\n",
    "        for img, mask in zip(image_patches, mask_patches):\n",
    "            # Apply transformations directly to numpy arrays\n",
    "            if self.train:  # Only apply augmentations during training\n",
    "                if random.random() > 0.5:\n",
    "                    img = np.flip(img, axis=1).copy()\n",
    "                    mask = np.flip(mask, axis=1).copy()\n",
    "                if random.random() > 0.5:\n",
    "                    img = np.flip(img, axis=0).copy()\n",
    "                    mask = np.flip(mask, axis=0).copy()\n",
    "                if random.random() > 0.5:\n",
    "                    k = random.choice([1, 2, 3])  # 90, 180, or 270 degrees\n",
    "                    img = np.rot90(img, k=k, axes=(0, 1)).copy()\n",
    "                    mask = np.rot90(mask, k=k, axes=(0, 1)).copy()\n",
    "\n",
    "            # Ensure the arrays are contiguous and in the correct range\n",
    "            img = np.ascontiguousarray(img)\n",
    "            mask = np.ascontiguousarray(mask)\n",
    "            \n",
    "            img = np.clip(img, 0, 1).astype(np.float32)\n",
    "            mask = mask.astype(np.uint8)\n",
    "\n",
    "            # Convert to tensor\n",
    "            img_tensor = torch.from_numpy(img.transpose(2, 0, 1))\n",
    "            mask_tensor = torch.from_numpy(mask).long()\n",
    "\n",
    "            transformed_images.append(img_tensor)\n",
    "            transformed_masks.append(mask_tensor)\n",
    "\n",
    "        return transformed_images, transformed_masks\n",
    "\n",
    "    def load_and_resize(self, path, is_mask=False):\n",
    "        data = rioxarray.open_rasterio(path).squeeze().values\n",
    "        \n",
    "        if not is_mask:\n",
    "            try:\n",
    "                data = preprocess_geospatial_data(data)\n",
    "            except Exception as e:\n",
    "                print(f\"Error preprocessing data from {path}: {str(e)}\")\n",
    "                # Return a default value or handle the error as appropriate\n",
    "                return np.zeros((1024, 1024), dtype=np.float32)\n",
    "        else:\n",
    "            data = (data > 0).astype(np.uint8)\n",
    "        \n",
    "        resized_image = cv2.resize(data, (1024, 1024), interpolation=cv2.INTER_NEAREST if is_mask else cv2.INTER_LANCZOS4)\n",
    "        \n",
    "        if is_mask:\n",
    "            resized_image = (resized_image > 0).astype(np.uint8)\n",
    "        else:\n",
    "            resized_image = resized_image.astype(np.float32)\n",
    "                \n",
    "        return resized_image\n",
    "    \n",
    "    def read_filenames_from_csv(self):\n",
    "        # Read filenames from CSV file\n",
    "        csv_path = os.path.join(self.csv_file)\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "\n",
    "    def extract_fire_id(self, path):\n",
    "        return int(os.path.basename(path).split('_')[0])\n",
    "\n",
    "    def split_dataset(self):\n",
    "        test_fids = [7123, 7792]\n",
    "        train_files = []\n",
    "        test_files = []\n",
    "\n",
    "        for _, row in self.data.iterrows():\n",
    "            fire_id = self.extract_fire_id(row['lum_change'])\n",
    "            if fire_id in test_fids:\n",
    "                test_files.append(row)\n",
    "            else:\n",
    "                train_files.append(row)\n",
    "        \n",
    "        if self.train:\n",
    "            self.dataset = train_files\n",
    "        else:\n",
    "            self.dataset = test_files\n",
    "    \n",
    "    def get_class_frequencies(self):\n",
    "        return self.class_frequencies\n",
    "\n",
    "def preprocess_geospatial_data(data):\n",
    "    # Replace NaN with a specific value, eg - the mean of non-NaN values\n",
    "    non_nan_mean = np.nanmean(data)\n",
    "    data = np.nan_to_num(data, nan=non_nan_mean)\n",
    "    \n",
    "    # Normalize the data\n",
    "    min_val, max_val = np.percentile(data, [1, 99])\n",
    "    \n",
    "    # Check if min_val and max_val are equal\n",
    "    if np.isclose(min_val, max_val):\n",
    "        return data.astype(np.float32)\n",
    "    \n",
    "    data = np.clip(data, min_val, max_val)\n",
    "    \n",
    "    # Add a small epsilon to avoid division by zero\n",
    "    epsilon = 1e-8\n",
    "    data = (data - min_val) / (max_val - min_val + epsilon)\n",
    "    \n",
    "    return data.astype(np.float32)\n",
    "\n",
    "def compute_iou(pred, target, num_classes):\n",
    "    pred = pred.cpu().numpy()\n",
    "    target = target.cpu().numpy()\n",
    "\n",
    "    iou_per_class = []\n",
    "    for cls in range(num_classes):\n",
    "        intersection = ((pred == cls) & (target == cls)).sum().item()\n",
    "        union = ((pred == cls) | (target == cls)).sum().item()\n",
    "        \n",
    "        if union != 0:\n",
    "            iou = intersection / union\n",
    "        else:\n",
    "            iou = 0.0\n",
    "        \n",
    "        iou_per_class.append(iou)\n",
    "    \n",
    "    mean_iou = sum(iou_per_class) / num_classes\n",
    "    return mean_iou\n",
    "\n",
    "def compute_accuracy(pred, target):\n",
    "    pred = pred.cpu().numpy()\n",
    "    target = target.cpu().numpy()\n",
    "\n",
    "    correct = (pred == target).sum().item()\n",
    "    total = target.size\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# Load pre-trained DeepLabV3 model\n",
    "model = models.deeplabv3_resnet50(weights=\"DEFAULT\", progress=True)\n",
    "\n",
    "# Replace the final layer with a new layer for binary classification\n",
    "num_classes = 2\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "    nn.BatchNorm2d(256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Conv2d(256, num_classes, kernel_size=(1, 1), stride=(1, 1))\n",
    ")\n",
    "\n",
    "# Modify the first convolutional layer if input channels changed\n",
    "if model.backbone.conv1.in_channels != 3:\n",
    "    model.backbone.conv1 = nn.Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "\n",
    "# Initialize only the new layers\n",
    "model.classifier.apply(init_weights)\n",
    "if model.backbone.conv1.in_channels != 3:\n",
    "    init_weights(model.backbone.conv1)\n",
    "\n",
    "# This one worked, but not with good test IoU or test accuracy\n",
    "# optimizer = optim.AdamW([\n",
    "#     {'params': model.backbone.parameters(), 'lr': 1e-5},\n",
    "#     {'params': model.classifier.parameters(), 'lr': 1e-4}\n",
    "# ], weight_decay=0.01)\n",
    "\n",
    "\n",
    "# 1. Adjust the learning rate\n",
    "optimizer = optim.AdamW([\n",
    "    {'params': model.backbone.parameters(), 'lr': 1e-4},\n",
    "    {'params': model.classifier.parameters(), 'lr': 1e-3}\n",
    "], weight_decay=0.01)\n",
    "\n",
    "# Add a learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "# Loss function\n",
    "criterion = RobustBCELoss()\n",
    "\n",
    "# Checkpoint loading\n",
    "checkpoint_dir = '/Bhaltos/ASHWATH/integrated_model_checkpoints_100m_v7/'\n",
    "latest_checkpoint = max(\n",
    "    [f for f in os.listdir(checkpoint_dir) if f.startswith('checkpoint_epoch_') and f.endswith('.pt')],\n",
    "    key=lambda f: int(f.split('_')[-1].replace('.pt', '')),\n",
    "    default=None\n",
    ")\n",
    "if latest_checkpoint:\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, latest_checkpoint)\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "    new_state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
    "    \n",
    "    model.load_state_dict(new_state_dict)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    best_loss = checkpoint['loss']\n",
    "    print(f\"Loaded checkpoint from epoch {start_epoch-1} with loss {best_loss}\")\n",
    "else:\n",
    "    start_epoch = 1\n",
    "    best_loss = float('inf')\n",
    "    print(\"No checkpoint found, starting training from scratch.\")\n",
    "\n",
    "# Define hyperparameters\n",
    "batch_size = 3\n",
    "accumulation_steps = 8\n",
    "num_epochs = 20\n",
    "checkpoint_freq = 1\n",
    "best_loss = float('inf')\n",
    "patience = 5\n",
    "early_stopping_counter = 0\n",
    "\n",
    "train_dataset = SegmentationGeotiffDataset(csv_file=\"/Bhaltos/ASHWATH/metadata_v2.csv\", train=True)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=6, pin_memory=True)\n",
    "\n",
    "test_dataset = SegmentationGeotiffDataset(csv_file=\"/Bhaltos/ASHWATH/metadata_v2.csv\", train=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=6, pin_memory=True)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    device_ids = [0, 1, 2]  # IDs of all available GPUs\n",
    "else:\n",
    "    device_ids = None\n",
    "if not isinstance(model, nn.DataParallel):\n",
    "    model = nn.DataParallel(model, device_ids=device_ids)\n",
    "model = model.to(device)\n",
    "\n",
    "# Function to enable gradient checkpointing\n",
    "def enable_gradient_checkpointing(model):\n",
    "    def checkpoint_sequential(module):\n",
    "        def custom_forward(*inputs):\n",
    "            for submodule in module.children():\n",
    "                inputs = submodule(*inputs)\n",
    "            return inputs\n",
    "        return lambda *x: checkpoint(custom_forward, *x)\n",
    "\n",
    "    # Apply checkpointing to ResNet layers in the backbone\n",
    "    if hasattr(model, 'backbone'):\n",
    "        for name, module in model.backbone.named_children():\n",
    "            if name.startswith('layer'):\n",
    "                setattr(model.backbone, name, checkpoint_sequential(module))\n",
    "\n",
    "    # Apply checkpointing to ASPP module\n",
    "    if hasattr(model, 'classifier') and hasattr(model.classifier, 'aspp'):\n",
    "        model.classifier.aspp = checkpoint_sequential(model.classifier.aspp)\n",
    "\n",
    "    return model\n",
    "\n",
    "# After model initialization and before training loop\n",
    "model = enable_gradient_checkpointing(model)\n",
    "\n",
    "# Create GradScaler for mixed precision training\n",
    "scaler = amp.GradScaler()\n",
    "\n",
    "# DataFrame to store metrics\n",
    "columns = ['Epoch', 'Train Loss', 'Test Loss', 'Test IoU', 'Test Accuracy']\n",
    "metrics_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(start_epoch, num_epochs+1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for i, (images, masks) in enumerate(tqdm(train_dataloader, desc=f\"Epoch {epoch}/{num_epochs} - Training\")):\n",
    "        \n",
    "        batch_loss = 0\n",
    "        for img_patches, mask_patches in zip(images, masks):\n",
    "            image_loss = 0\n",
    "            for img, mask in zip(img_patches, mask_patches):\n",
    "                img = img.unsqueeze(0).to(device)\n",
    "                mask = mask.unsqueeze(0).to(device)\n",
    "\n",
    "                with amp.autocast():\n",
    "                    output = model(img)['out']\n",
    "                    output_logits = output[:, 1]  # Use logits directly\n",
    "                    patch_loss = criterion(output_logits, mask.float())\n",
    "                \n",
    "                scaler.scale(patch_loss).backward()\n",
    "\n",
    "                image_loss += patch_loss\n",
    "\n",
    "                # Clear unnecessary memory\n",
    "                del img, mask, output, output_logits\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            # Average loss for all patches in the image\n",
    "            image_loss /= len(img_patches)\n",
    "            batch_loss += image_loss\n",
    "\n",
    "            #scaler.scale(image_loss).backward()\n",
    "        \n",
    "        # Average loss for all images in the batch\n",
    "        batch_loss /= len(images)\n",
    "        batch_loss = batch_loss / accumulation_steps\n",
    "\n",
    "        # Use scaler for mixed precision training\n",
    "        #scaler.scale(batch_loss).backward()\n",
    "        #batch_loss.backward()\n",
    "\n",
    "        if (i + 1) % accumulation_steps == 0 or (i + 1) == len(train_dataloader):\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            #optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        running_loss += batch_loss.item() * accumulation_steps\n",
    "    \n",
    "    avg_train_loss = running_loss / len(train_dataloader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    iou_scores = []\n",
    "    accuracies = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(test_dataloader, desc=f\"Epoch {epoch}/{num_epochs} - Validation\"):\n",
    "            batch_loss = 0\n",
    "            batch_iou = 0\n",
    "            batch_accuracy = 0\n",
    "            \n",
    "            for img_patches, mask_patches in zip(images, masks):\n",
    "                image_predictions = []\n",
    "                image_masks = []\n",
    "                image_loss = 0\n",
    "                \n",
    "                for img, mask in zip(img_patches, mask_patches):\n",
    "                    img = img.unsqueeze(0).to(device)\n",
    "                    mask = mask.unsqueeze(0).to(device)\n",
    "                    \n",
    "                    with amp.autocast():\n",
    "                        output = model(img)['out']\n",
    "                        output_logits = output[:, 1]\n",
    "                        patch_loss = criterion(output_logits, mask.float())\n",
    "                    \n",
    "                    image_loss += patch_loss.item()\n",
    "                    output_pred = (torch.sigmoid(output_logits) > 0.5).long()\n",
    "                    \n",
    "                    image_predictions.append(output_pred.cpu())\n",
    "                    image_masks.append(mask.cpu())\n",
    "\n",
    "                    # Clear unnecessary memory\n",
    "                    del img, mask, output, output_logits\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                image_loss /= len(img_patches)\n",
    "                image_pred = torch.cat(image_predictions, dim=0)\n",
    "                image_mask = torch.cat(image_masks, dim=0)\n",
    "                \n",
    "                # Calculate metrics for the entire image\n",
    "                image_iou = compute_iou(image_pred, image_mask, num_classes)\n",
    "                image_accuracy = compute_accuracy(image_pred, image_mask)\n",
    "                \n",
    "                batch_loss += image_loss\n",
    "                batch_iou += image_iou\n",
    "                batch_accuracy += image_accuracy\n",
    "            \n",
    "            # Average metrics for the batch\n",
    "            batch_loss /= len(images)\n",
    "            batch_iou /= len(images)\n",
    "            batch_accuracy /= len(images)\n",
    "            \n",
    "            test_loss += batch_loss\n",
    "            iou_scores.append(batch_iou)\n",
    "            accuracies.append(batch_accuracy)\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_dataloader)\n",
    "    avg_iou = np.mean(iou_scores)\n",
    "    avg_accuracy = np.mean(accuracies)\n",
    "\n",
    "    # Update metrics DataFrame\n",
    "    new_row = pd.DataFrame({\n",
    "    'Epoch': [epoch],\n",
    "    'Train Loss': [avg_train_loss],\n",
    "    'Test Loss': [avg_test_loss],\n",
    "    'Test IoU': [avg_iou],\n",
    "    'Test Accuracy': [avg_accuracy]\n",
    "    })\n",
    "    metrics_df = pd.concat([metrics_df, new_row], ignore_index=True)\n",
    "\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}] - Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}, IoU: {avg_iou:.4f}, Accuracy: {avg_accuracy:.4f}\")\n",
    "\n",
    "    # Check for early stopping\n",
    "    if avg_test_loss < best_loss:\n",
    "        best_loss = avg_test_loss\n",
    "        early_stopping_counter = 0\n",
    "        \n",
    "        # Save the model checkpoint\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch}.pt')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': best_loss\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        if early_stopping_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Save metrics to CSV\n",
    "metrics_df.to_csv('/Bhaltos/ASHWATH/integrated_100m_training_metrics_v7.csv', index=False)\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset Statistics:\n",
      "Total pixels: 174063616\n",
      "Class 0 pixels: 103274130 (59.33%)\n",
      "Class 1 pixels: 70789486 (40.67%)\n",
      "\n",
      "Test Dataset Statistics:\n",
      "Total pixels: 42991616\n",
      "Class 0 pixels: 16662413 (38.76%)\n",
      "Class 1 pixels: 26329203 (61.24%)\n"
     ]
    }
   ],
   "source": [
    "def print_dataset_stats(dataset):\n",
    "    total_pixels = 0\n",
    "    class_pixels = {0: 0, 1: 0}\n",
    "    for _, masks in dataset:\n",
    "        for mask in masks:\n",
    "            total_pixels += mask.numel()\n",
    "            class_pixels[0] += (mask == 0).sum().item()\n",
    "            class_pixels[1] += (mask == 1).sum().item()\n",
    "    print(f\"Total pixels: {total_pixels}\")\n",
    "    print(f\"Class 0 pixels: {class_pixels[0]} ({class_pixels[0]/total_pixels:.2%})\")\n",
    "    print(f\"Class 1 pixels: {class_pixels[1]} ({class_pixels[1]/total_pixels:.2%})\")\n",
    "\n",
    "# Call this function after creating your datasets\n",
    "print(\"Train Dataset Statistics:\")\n",
    "print_dataset_stats(train_dataset)\n",
    "print(\"\\nTest Dataset Statistics:\")\n",
    "print_dataset_stats(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Train Loss</th>\n",
       "      <th>Test Loss</th>\n",
       "      <th>Test IoU</th>\n",
       "      <th>Test Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.992057</td>\n",
       "      <td>0.998107</td>\n",
       "      <td>0.498618</td>\n",
       "      <td>0.996358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.982693</td>\n",
       "      <td>0.992132</td>\n",
       "      <td>0.496132</td>\n",
       "      <td>0.985884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.976462</td>\n",
       "      <td>0.983806</td>\n",
       "      <td>0.478979</td>\n",
       "      <td>0.943626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.967252</td>\n",
       "      <td>0.993409</td>\n",
       "      <td>0.426016</td>\n",
       "      <td>0.847876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.964017</td>\n",
       "      <td>0.996425</td>\n",
       "      <td>0.500336</td>\n",
       "      <td>0.997149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.959947</td>\n",
       "      <td>0.984929</td>\n",
       "      <td>0.508443</td>\n",
       "      <td>0.990969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Epoch  Train Loss  Test Loss  Test IoU  Test Accuracy\n",
       "0      1    0.992057   0.998107  0.498618       0.996358\n",
       "1      2    0.982693   0.992132  0.496132       0.985884\n",
       "2      3    0.976462   0.983806  0.478979       0.943626\n",
       "3      4    0.967252   0.993409  0.426016       0.847876\n",
       "4      5    0.964017   0.996425  0.500336       0.997149\n",
       "5      6    0.959947   0.984929  0.508443       0.990969"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/Bhaltos/ASHWATH/integrated_100m_training_metrics.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Train Loss</th>\n",
       "      <th>Test Loss</th>\n",
       "      <th>Test IoU</th>\n",
       "      <th>Test Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.993277</td>\n",
       "      <td>0.995113</td>\n",
       "      <td>0.010434</td>\n",
       "      <td>0.023463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.988467</td>\n",
       "      <td>0.992578</td>\n",
       "      <td>0.406270</td>\n",
       "      <td>0.813274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.985425</td>\n",
       "      <td>0.995456</td>\n",
       "      <td>0.498539</td>\n",
       "      <td>0.997085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.984510</td>\n",
       "      <td>0.987235</td>\n",
       "      <td>0.492394</td>\n",
       "      <td>0.984871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.975766</td>\n",
       "      <td>0.985969</td>\n",
       "      <td>0.492613</td>\n",
       "      <td>0.985247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.968253</td>\n",
       "      <td>0.999685</td>\n",
       "      <td>0.498690</td>\n",
       "      <td>0.997381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.963056</td>\n",
       "      <td>0.977673</td>\n",
       "      <td>0.495169</td>\n",
       "      <td>0.990415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.959055</td>\n",
       "      <td>0.999595</td>\n",
       "      <td>0.498688</td>\n",
       "      <td>0.997376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.958593</td>\n",
       "      <td>0.978879</td>\n",
       "      <td>0.497337</td>\n",
       "      <td>0.994687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.955447</td>\n",
       "      <td>0.999560</td>\n",
       "      <td>0.498692</td>\n",
       "      <td>0.997385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Epoch  Train Loss  Test Loss  Test IoU  Test Accuracy\n",
       "0      1    0.993277   0.995113  0.010434       0.023463\n",
       "1      2    0.988467   0.992578  0.406270       0.813274\n",
       "2      3    0.985425   0.995456  0.498539       0.997085\n",
       "3      4    0.984510   0.987235  0.492394       0.984871\n",
       "4      5    0.975766   0.985969  0.492613       0.985247\n",
       "5      6    0.968253   0.999685  0.498690       0.997381\n",
       "6      7    0.963056   0.977673  0.495169       0.990415\n",
       "7      8    0.959055   0.999595  0.498688       0.997376\n",
       "8      9    0.958593   0.978879  0.497337       0.994687\n",
       "9     10    0.955447   0.999560  0.498692       0.997385"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/Bhaltos/ASHWATH/integrated_100m_training_metrics_v2.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Train Loss</th>\n",
       "      <th>Test Loss</th>\n",
       "      <th>Test IoU</th>\n",
       "      <th>Test Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.887011</td>\n",
       "      <td>0.846563</td>\n",
       "      <td>0.49869</td>\n",
       "      <td>0.997381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.846562</td>\n",
       "      <td>0.846563</td>\n",
       "      <td>0.49869</td>\n",
       "      <td>0.997381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.846561</td>\n",
       "      <td>0.846563</td>\n",
       "      <td>0.49869</td>\n",
       "      <td>0.997381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.846562</td>\n",
       "      <td>0.846563</td>\n",
       "      <td>0.49869</td>\n",
       "      <td>0.997381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.846562</td>\n",
       "      <td>0.846563</td>\n",
       "      <td>0.49869</td>\n",
       "      <td>0.997381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.846562</td>\n",
       "      <td>0.846563</td>\n",
       "      <td>0.49869</td>\n",
       "      <td>0.997381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Epoch  Train Loss  Test Loss  Test IoU  Test Accuracy\n",
       "0      1    0.887011   0.846563   0.49869       0.997381\n",
       "1      2    0.846562   0.846563   0.49869       0.997381\n",
       "2      3    0.846561   0.846563   0.49869       0.997381\n",
       "3      4    0.846562   0.846563   0.49869       0.997381\n",
       "4      5    0.846562   0.846563   0.49869       0.997381\n",
       "5      6    0.846562   0.846563   0.49869       0.997381"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/Bhaltos/ASHWATH/integrated_100m_training_metrics_v3.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Train Loss</th>\n",
       "      <th>Test Loss</th>\n",
       "      <th>Test IoU</th>\n",
       "      <th>Test Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.433625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.189173</td>\n",
       "      <td>0.378346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.344401</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.189173</td>\n",
       "      <td>0.378346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.344401</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.215774</td>\n",
       "      <td>0.427972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.344401</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.189173</td>\n",
       "      <td>0.378346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.351361</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.189173</td>\n",
       "      <td>0.378346</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Epoch  Train Loss  Test Loss  Test IoU  Test Accuracy\n",
       "0      1    0.433625        NaN  0.189173       0.378346\n",
       "1      2    0.344401        NaN  0.189173       0.378346\n",
       "2      3    0.344401        NaN  0.215774       0.427972\n",
       "3      4    0.344401        NaN  0.189173       0.378346\n",
       "4      5    0.351361        NaN  0.189173       0.378346"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/Bhaltos/ASHWATH/integrated_100m_training_metrics_v4.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Train Loss</th>\n",
       "      <th>Test Loss</th>\n",
       "      <th>Test IoU</th>\n",
       "      <th>Test Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.716864</td>\n",
       "      <td>0.691347</td>\n",
       "      <td>0.225756</td>\n",
       "      <td>0.419199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.695056</td>\n",
       "      <td>0.692915</td>\n",
       "      <td>0.189177</td>\n",
       "      <td>0.378354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693113</td>\n",
       "      <td>0.189177</td>\n",
       "      <td>0.378354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.692090</td>\n",
       "      <td>0.189177</td>\n",
       "      <td>0.378354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.690490</td>\n",
       "      <td>0.189177</td>\n",
       "      <td>0.378354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693105</td>\n",
       "      <td>0.189177</td>\n",
       "      <td>0.378354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693116</td>\n",
       "      <td>0.189177</td>\n",
       "      <td>0.378354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.692999</td>\n",
       "      <td>0.189177</td>\n",
       "      <td>0.378354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.692795</td>\n",
       "      <td>0.189177</td>\n",
       "      <td>0.378354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.691509</td>\n",
       "      <td>0.189177</td>\n",
       "      <td>0.378354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Epoch  Train Loss  Test Loss  Test IoU  Test Accuracy\n",
       "0      1    0.716864   0.691347  0.225756       0.419199\n",
       "1      2    0.695056   0.692915  0.189177       0.378354\n",
       "2      3    0.693147   0.693113  0.189177       0.378354\n",
       "3      4    0.693147   0.692090  0.189177       0.378354\n",
       "4      5    0.693147   0.690490  0.189177       0.378354\n",
       "5      6    0.693147   0.693105  0.189177       0.378354\n",
       "6      7    0.693147   0.693116  0.189177       0.378354\n",
       "7      8    0.693147   0.692999  0.189177       0.378354\n",
       "8      9    0.693147   0.692795  0.189177       0.378354\n",
       "9     10    0.693147   0.691509  0.189177       0.378354"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/Bhaltos/ASHWATH/integrated_100m_training_metrics_v5.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
